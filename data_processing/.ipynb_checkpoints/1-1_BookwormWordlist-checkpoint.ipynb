{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Step 1: Processing Feature Files for Bookworm\n",
    "\n",
    "This notebook runs through Extracted Features files, saving:\n",
    "\n",
    "1. Global token counts (by language) toward the eventual Bookworm Wordlist. \n",
    "    These aren't all folded here: rather, they are folded by batch and saved to an HDF5Store.\n",
    "    Later, they'll all be folded into one big list. *This is a big process that you should never have to do again: just reuse the Word Lists.*\n",
    "    - Next Step: `BookwormFoldWordList.ipynb` > `CreateWordlist.ipynb`\n",
    "\n",
    "2. \"Raw\" unigram counts per book. These will eventually be trimmed to only the BW vocabulary and\n",
    "    labelled by an id. This information first needs the wordlist that #1 above will create, but\n",
    "    since we're already opening the EF files, might as well do some processing and save this\n",
    "    intermediate state to a fast IO format (HDF5 store, again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader, utils\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook # Progress bars!\n",
    "from ipyparallel import Client\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before attaching to ipyparallel engines, they need to be started with \n",
    "\n",
    "```bash\n",
    "    ipcluster start -n NUM\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = Client()\n",
    "dview = rc[:]\n",
    "v = rc.load_balanced_view()\n",
    "\n",
    "# Need this later to force garbage collection\n",
    "@dview.remote(block=True)\n",
    "def force_gc():\n",
    "    import gc\n",
    "    before = gc.get_count()\n",
    "    gc.collect()\n",
    "    return before[0], gc.get_count()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize logging. There's no nice way to pass logs between engines, so just give each one its own log.\n",
    "\n",
    "The timestamp format is designed for easy sort, so you can track all logs with \n",
    "\n",
    "```bash\n",
    "watch \"tail -q -n 100 logs/* | sort\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init_log(name=False):\n",
    "    import logging, os\n",
    "    if not name:\n",
    "        name = os.getpid()\n",
    "    handler = logging.FileHandler(\"/notebooks/data/logs/bw-%s.log\" % name, 'a')\n",
    "    formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s', \"%m/%d-%H:%M:%S\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "    logging.info(\"Log initialized\")\n",
    "\n",
    "dview.push(dict(init_log=init_log))\n",
    "init_log(\"root\")\n",
    "%px init_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load paths to feature files. This notebook maintains a list of successully processed ids, so there are some functions that help us cross reference all volumes with done volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts 1000\n"
     ]
    }
   ],
   "source": [
    "with open(\"/notebooks/features/listing/ids.txt\", \"r\") as f:\n",
    "    paths = [\"/notebooks/features/\"+path.strip() for path in f.readlines()][1:]\n",
    "    print(\"Number of texts\", len(paths))\n",
    "\n",
    "successfile = \"/notebooks/data/successful-counts.txt\"\n",
    "def get_processed():\n",
    "    import numpy as np\n",
    "    ''' Get already processed files. Wrapped in func for easy refresh'''\n",
    "    try:\n",
    "        with open(successfile, \"r\") as f:\n",
    "            paths = f.read().strip().split(\"\\n\")\n",
    "        paths = [\"/notebooks/features/\"+utils.id_to_rsync(path) for path in paths]\n",
    "        return np.array(paths)\n",
    "    except:\n",
    "        return np.array([])\n",
    "\n",
    "path_to_id = lambda x: x.replace(\".json.bz2\", \"\").split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_count` is the function that does the processing of the volume. To improve performance, however, the subprocesses run larger volumes in larger batches with `get_doc_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/features/mdp/31654/mdp.39015066356547.json.bz2\n",
      "<Volume: Slovenské vyst̕ahovalectvo / (1969) by Bielik, František,>\n",
      "mdp.39015066356547\n",
      "['slo', 'ger', 'hun', 'fre', 'spa']\n",
      "<class 'list'>\n",
      "                                   count\n",
      "language id                 token       \n",
      "slo      mdp.39015066356547 !        145\n",
      "                            !\"         3\n",
      "                            !)         1\n",
      "                            !2         1\n",
      "                            \"        210\n",
      "...                                  ...\n",
      "                            ■          1\n",
      "                            ■*         1\n",
      "                            ■/.        1\n",
      "                            ■■         1\n",
      "                            □          1\n",
      "\n",
      "[36452 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">slo</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">mdp.39015066356547</th>\n",
       "      <th>!</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!\"</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!)</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   count\n",
       "language id                 token       \n",
       "slo      mdp.39015066356547 !        145\n",
       "                            !\"         3\n",
       "                            !)         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trim_token(t, max=50):\n",
    "    ''' Trim unicode string to max number of bytes'''\n",
    "    if len(t.encode('utf-8')) > max:\n",
    "        while len(t.encode('utf-8')) > max:\n",
    "            t = t[:-1]\n",
    "    return t\n",
    "\n",
    "def get_count(path, store=False):\n",
    "    ''' Get tokencount information from a single doc, by path'''\n",
    "    from htrc_features import FeatureReader    \n",
    "    max_char = 50\n",
    "    print(path)\n",
    "    vol = FeatureReader(path).first()\n",
    "    print(vol)\n",
    "    print(vol.id)\n",
    "    print(vol.language)\n",
    "    print(type(vol.language))\n",
    "    tl = vol.tokenlist(pages=False, pos=False)\n",
    "    if tl.empty:\n",
    "        return tl\n",
    "    else:\n",
    "        tl = tl.reset_index('section')[['count']]\n",
    "    tl.index = [trim_token(t, max_char) for t in tl.index.values]\n",
    "    tl.index.names=['token']\n",
    "    tl['id'] = vol.id\n",
    "    if type(vol.language) is list:\n",
    "        tl['language'] = vol.language[0]\n",
    "    else:\n",
    "        tl['language'] = vol.language\n",
    "    tl = tl.reset_index('token').set_index(['language', 'id', 'token']).sort_index()\n",
    "    print(tl)\n",
    "    return tl\n",
    "\n",
    "# Send to Engines\n",
    "dview.push(dict(trim_token=trim_token, get_count=get_count))\n",
    "\n",
    "# Example\n",
    "get_count(paths[0]).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_counts(paths, mincount=False, max_str_bytes = 50):\n",
    "    '''\n",
    "    This method lets you process multiple paths at a time on a single engine.\n",
    "    This means the engine can collect enough texts to do a simple filter (i.e. >X counts in Y texts)\n",
    "    and can save to it's own store.\n",
    "    '''\n",
    "    import logging\n",
    "    import os\n",
    "    import gc\n",
    "    import pandas as pd\n",
    "    fname = '/notebooks/data/stores/bw_counts_%s.h5' % os.getpid()\n",
    "    success_log = []\n",
    "    logging.info(\"Starting %d volume batch on PID=%s\" % (len(paths), os.getpid()))\n",
    "    with pd.HDFStore(fname, mode=\"a\", complevel=9, complib='blosc') as store:\n",
    "        tl_collector = []\n",
    "        for path in paths:\n",
    "            logging.info(\"Trying %s\" % path)\n",
    "            logging.info(\"%f MB\" % (os.stat(path).st_size / (1024.0 * 1024)))\n",
    "            try:\n",
    "                tl = get_count(path, store=store)\n",
    "                if tl.empty:\n",
    "                    logging.info(\"%s is empty\" % path)\n",
    "                    continue\n",
    "                tl_collector.append(tl)\n",
    "            except:\n",
    "                logging.exception(\"Unable to get count for path %s\" % path)\n",
    "                continue\n",
    "            success_log.append(path)\n",
    "\n",
    "        # Save a DF combining all the counts from this batch\n",
    "        try:\n",
    "            logging.info(\"Merging and Saving texts for %d paths starting with %s\" % (len(paths), paths[0]))\n",
    "            combineddf = pd.concat(tl_collector)\n",
    "            logging.info(\"Created combined df\")\n",
    "            \n",
    "            # Save tf(doc) with volid but no lang\n",
    "            # For efficient HDF5 storage, enforcing a 50 byte token limit. Can't use\n",
    "            # DataFrame.str.slice(stop=50) though, because really we care about bytes and \n",
    "            # some unicode chars are multiple codepoints.\n",
    "            # volids are capped at 25chars (the longest PD vol id)\n",
    "            combineddf_by_volid = combineddf.reset_index('language')[['count']]\n",
    "            logging.info(\"Reset index\")\n",
    "            logging.info(\"Number of rows: %d\" % len(combineddf.index))\n",
    "            logging.info(\"Size of DataFrame: %f MB\" % (combineddf.memory_usage(deep=True).sum() / (1024 * 1024)))\n",
    "#            logging.info(\"%s\" % combineddf.info(memory_usage='deep'))\n",
    "            store.append('/tf/docs',\n",
    "                         combineddf_by_volid,\n",
    "                         min_itemsize = {'id': 25, 'token':max_str_bytes})\n",
    "            logging.info(\"Appended volid and token\")\n",
    "            \n",
    "            ### Save tf(corpus)\n",
    "            df = combineddf.groupby(level=['language', 'token'])[['count']]\\\n",
    "                           .sum().sort_index()\n",
    "            logging.info(\"Grouped by language\")\n",
    "            # Filtering this way (by corpus total, not language total) is too slow:\n",
    "            #if mincount:\n",
    "            #    df = df.groupby(level='token')[['count']].filter(lambda x: x.sum()>=mincount)\n",
    "            # Because we can't feasibly filter on total count and have to do so by lang x token, it\n",
    "            # might unfairly punish sparse languages. My workaround is to only even trim English by\n",
    "            # mincount: any bias this would have would be in the bottom of the wordlist anyway.\n",
    "            if mincount:\n",
    "                df = df[(df.index.get_level_values(0) != 'eng') | (df['count']>2)]\n",
    "            store.append('tf/corpus', df, min_itemsize = {'token': max_str_bytes})\n",
    "            logging.info(\"Appended language and token\")\n",
    "            tl_collector = dict()\n",
    "            return success_log\n",
    "        except:\n",
    "            logging.exception(\"Saving error for %d paths starting with %s\" % (len(paths), paths[0]))\n",
    "            return []\n",
    "    gc.collect()\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 paths remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-35e5f07f2f06>:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for result in tqdm_notebook(parallel_job, smoothing=0):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61849faa9d8e40edbbfa49337227820e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=40.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Split paths into N-sized chunks, so engines can iterate on multiple texts at once\n",
    "chunk_size = 25\n",
    "remaining_paths = np.setdiff1d(paths, get_processed())\n",
    "print(\"%d paths remaining\" % len(remaining_paths))\n",
    "n = 10000000\n",
    "start = 0\n",
    "chunked_paths = [remaining_paths[start+i:start+i+chunk_size] for i in range(0, len(remaining_paths[start:start+n]), chunk_size)]\n",
    "\n",
    "starttime = time.time()\n",
    "logging.info(\"Starting parallel job\")\n",
    "parallel_job = v.map(get_doc_counts, chunked_paths, ordered=False)\n",
    "\n",
    "i = 0\n",
    "for result in tqdm_notebook(parallel_job, smoothing=0):\n",
    "    i += 1\n",
    "    if result:\n",
    "        with open(successfile, \"a+\") as f:\n",
    "            ids = [path_to_id(path) for path in result]\n",
    "            f.write(\"\\n\".join(ids)+\"\\n\")\n",
    "        logging.info(\"Done processing batch %d, from %s to %s\" % (i, result[0], result[-1]))\n",
    "    else:\n",
    "        logging.error(\"Problem with result in batch %d\" % i)\n",
    "\n",
    "force_gc()\n",
    "logging.info(\"Done\")\n",
    "logging.info(time.time()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "- Check for duplicates in \"successful-counts.txt\". I caught one text duplicated due to a bug, good to check that it doesn't happen again.\n",
    "- Create a table index after storage (e.g. `store.create_table_index('df', optlevel=9, kind='full')`)\n",
    "\n",
    "## Notes\n",
    "- Future merges need to be at uint64 or int64, because uint32 is too small. For some reason, PyTables doesn't allow uint64 data columns, so int64 is used solely for that reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "\n",
    "## Count up unique volume ids from stores\n",
    "\n",
    "Useful in the case I ran into where the ZMQ connect between the root and nodes broke, so I wasn't saving the list of successfully processed volumes, but the Engines were still happily crunching away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from ipyparallel import require\n",
    "storestocheck = glob.glob(\"/notebooks/data/stores/*h5\")\n",
    "\n",
    "@require(get_processed)\n",
    "def check_for_processed(storefile):\n",
    "    import gc\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import logging\n",
    "    from htrc_features import utils\n",
    "    \n",
    "    all_unique = []\n",
    "    batchsize = 100000000\n",
    "    \n",
    "    with pd.HDFStore(storefile, mode=\"r\") as store:\n",
    "        # Rejecting files where the last row was not mdp\n",
    "        try:\n",
    "            print(\"a\")\n",
    "            n = int(store.get_storer(\"/tf/docs\").nrows)\n",
    "        except:\n",
    "            print(\"b\")\n",
    "            logging.exception(\"Can't get row count for %s, moving on\" % storefile)\n",
    "            return []\n",
    "        try:\n",
    "            print(\"c\")\n",
    "            a = store.select_column('/tf/docs', 'id', start=n-2)\n",
    "            if a.str.split(\".\")[0][0] != 'mdp':\n",
    "                logging.info(\"%s didn't process mdp most recently, skipping.\" % storefile)\n",
    "                return []\n",
    "        except:\n",
    "            print(\"d\")\n",
    "            logging.exception(\"Error with %s\" % storefile)\n",
    "            return []\n",
    "\n",
    "        print(\"e\")\n",
    "        logging.info(\"Figuring out what is already processed.\")\n",
    "        already_processed = get_processed()\n",
    "\n",
    "        logging.info(\"Going through file backwards until all the volume ids are in the success list\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                logging.info(\"Processing %s from %d\" % (storefile, n-batchsize))\n",
    "                startrow = (n - batchsize) if n > batchsize else 0\n",
    "                unique = store.select_column('/tf/docs', 'id', start=startrow, stop=n).unique()\n",
    "                uniquemdp = unique[np.char.startswith(unique.astype(np.unicode), \"mdp\")]\n",
    "                as_paths =  pd.Series(uniquemdp).apply(lambda x: '/notebooks/features/' + utils.id_to_rsync(x)).values\n",
    "                \n",
    "                to_process = np.setdiff1d(as_paths, already_processed)\n",
    "                if to_process.shape[0] == 0:\n",
    "                    logging.info(\"Done at %d\" % (n-batchsize))\n",
    "                    break\n",
    "                else:\n",
    "                    n -= batchsize\n",
    "                    all_unique.append(to_process)\n",
    "            except:\n",
    "                n -= batchsize\n",
    "                logging.exception(\"Error with %s from %d)\" % (storefile, n))\n",
    "            try:\n",
    "                gc.collect()\n",
    "            except:\n",
    "                logging.exception(\"gc error\")\n",
    "    if len(all_unique) > 0:\n",
    "        try:\n",
    "            return np.unique(np.concatenate(all_unique))\n",
    "        except:\n",
    "            logging.exception(\"problem with array concatenatation, returning list\")\n",
    "            return all_unique\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick store check\n",
    "\n",
    "Grab the last item from each store. This is a good way to check if a store broke for whatever reason.\n",
    "\n",
    "The ptrepack command on your system seems to repack the non-corrupted part of the file, at least until it hits the error. That will be incomplete, but at least you have something that isn't crashing processes down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/data/stores/bw_counts_80.h5\n",
      "0    uva.x001645091\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_81.h5\n",
      "0    wu.89077892479\n",
      "Name: id, dtype: object\n",
      "/notebooks/data/stores/bw_counts_83.h5\n",
      "0    uva.x030554918\n",
      "Name: id, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0    uva.x001645091\n",
       " Name: id, dtype: object,\n",
       " 0    wu.89077892479\n",
       " Name: id, dtype: object,\n",
       " 0    uva.x030554918\n",
       " Name: id, dtype: object]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "storestocheck = glob.glob(\"/notebooks/data/stores/*h5\")\n",
    "def get_last(storefile):\n",
    "    import pandas as pd\n",
    "    with pd.HDFStore(storefile, mode=\"a\") as store:\n",
    "        n = int(store.get_storer(\"/tf/docs\").nrows)\n",
    "        return store.select_column('/tf/docs', 'id', start=n-1)\n",
    "\n",
    "last = []\n",
    "for store in storestocheck:\n",
    "    print(store)\n",
    "    last.append(get_last(store))\n",
    "    print(last[-1])\n",
    "last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-bdc422de31a6>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for ids in tqdm_notebook(parallel_job, smoothing=0):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dcbc2fc1ed34c458320462b8fe32bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dview.push(dict(successfile=successfile, get_processed=get_processed))\n",
    "parallel_job = v.map(check_for_processed, storestocheck, ordered=False)\n",
    "all_ids = []\n",
    "i = 0\n",
    "for ids in tqdm_notebook(parallel_job, smoothing=0):\n",
    "    all_ids.append(ids)\n",
    "    i += 1\n",
    "    logging.info(\"Batch %d done\" % i)\n",
    "\n",
    "uniqueids = np.unique(np.concatenate(all_ids))\n",
    "\n",
    "np.save(\"addtosuccessful2\", uniqueids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series(uniqueids)\n",
    "b = a[a.astype(str).str.find(\"mdp\") >= 0]\n",
    "c = get_processed()\n",
    "d = np.setdiff1d(b.values, c)\n",
    "e = pd.Series(d).apply(lambda x: x.split(\"/\")[-1].split(\".json\")[0]).values\n",
    "with open(successfile, \"a+\") as f:\n",
    "    f.write(\"\\n\".join(e)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_paths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_paths = np.setdiff1d(paths, get_processed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_paths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(48, 0), (563, 0), (44, 0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "force_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del parallel_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del storestocheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del chunked_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
