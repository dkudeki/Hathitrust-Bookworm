{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Wordlist\n",
    "\n",
    "Create a vocabulary list for Bookworm, optimized to include top language-specific tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009112454950809479"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "st = os.stat('/notebooks/data/final/final-sorted.h5')\n",
    "st.st_size / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/eng    123771\n",
       "/hin     44774\n",
       "/jpn     19241\n",
       "/slo      6335\n",
       "/chi      3540\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all langs and their sizes\n",
    "with pd.HDFStore('/notebooks/data/final/final-sorted.h5') as store:\n",
    "    keys = store.keys()\n",
    "    sizes = [store.get_storer(key).nrows for key in keys]\n",
    "tablesizes = pd.Series(sizes, index=keys).sort_values(ascending=False)\n",
    "tablesizes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'No object named /gre in the file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-92b1674566ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/notebooks/data/final/final-sorted.h5'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ελλάδα'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No object named {key} in the file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;31m# create the storer and axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'No object named /gre in the file'"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore('/notebooks/data/final/final-sorted.h5') as store:\n",
    "    df = store.select('/gre')\n",
    "df[df.index.str.startswith('Ελλάδα')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1aff4a6694f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ελλάδα'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[df.index.str.startswith('Ελλάδα')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine a trimming policy for each lang\n",
    "\n",
    "Each language contributes top $N_{lang}$ tokens to the word list. $N_{lang}$ is selected according to the following rules:\n",
    "\n",
    "    4% of the language's saved vocabulary, to a minimum of 25k, and hard-coded adjustments for the biggest languages where 4% is too high (eng=900m, ger=650k, {fre,lat,rus}=400k, {jpn,ita,spa}=250k). Any language with less than 100k tokens *total* is assumed to be a junk language, or one that BW is not useful for to begin with, so it's trimmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens (including possible dupes) 1850000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>count</th>\n",
       "      <th>retain_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/eng</td>\n",
       "      <td>123771</td>\n",
       "      <td>900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/hin</td>\n",
       "      <td>44774</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/jpn</td>\n",
       "      <td>19241</td>\n",
       "      <td>300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/slo</td>\n",
       "      <td>6335</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/chi</td>\n",
       "      <td>3540</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/ger</td>\n",
       "      <td>2075</td>\n",
       "      <td>650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/und</td>\n",
       "      <td>389</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/ind</td>\n",
       "      <td>287</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lang   count  retain_count\n",
       "0  /eng  123771        900000\n",
       "1  /hin   44774             0\n",
       "2  /jpn   19241        300000\n",
       "3  /slo    6335             0\n",
       "4  /chi    3540             0\n",
       "5  /ger    2075        650000\n",
       "6  /und     389             0\n",
       "7  /ind     287             0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference for how many top words to keep\n",
    "top_words_ref = dict(eng=900000, ger=650000,\n",
    "                     fre=400000, lat=360000, rus=280000,\n",
    "                     jpn=300000, ita=220000, spa=220000)\n",
    "\n",
    "def trim_topwords(row):\n",
    "    if row[0][1:] in top_words_ref:\n",
    "        return top_words_ref[row[0][1:]]\n",
    "    elif row[1] < 100000:\n",
    "        # Ignore langs with practically no words as likely duds, or at the very least\n",
    "        # something BW wouldn't be useful for\n",
    "        return 0\n",
    "    else:\n",
    "        # Other languages: keep greater of 25k or 5% of vocab\n",
    "        mincount = 20000\n",
    "        percentagetrim = int(row[1] * 0.035)\n",
    "        return percentagetrim if percentagetrim > mincount else mincount\n",
    "\n",
    "cutoff_list = tablesizes.reset_index().rename(columns={'index': 'lang', 0: 'count'})\n",
    "cutoff_list['retain_count'] = cutoff_list.apply(trim_topwords, axis=1)\n",
    "print(\"Total tokens (including possible dupes)\", cutoff_list['retain_count'].sum())\n",
    "cutoff_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "problem_dfs = []\n",
    "for i, row in cutoff_list.iterrows():\n",
    "    if row['retain_count'] == 0:\n",
    "        continue\n",
    "    df = pd.read_hdf('/notebooks/data/final/final-sorted.h5', row['lang'], stop=1000000)\n",
    "    \n",
    "    count = df[(df.index.str.startswith('\\u200b') | df.index.str.endswith(\"\\u200b\"))]\n",
    "    if count.shape[0] != 0:\n",
    "        print(\"%s has %d tokens withs \\\\u200b in the top 1m\" % (row['lang'], count.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [count]\n",
      "Index: []\n",
      "Final wordlist using top N trim criteria:  (141924, 1)\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "problem_dfs = []\n",
    "for i, row in cutoff_list.iterrows():\n",
    "    if row['retain_count'] == 0:\n",
    "        continue\n",
    "    df = pd.read_hdf('/notebooks/data/final/final-sorted.h5', row['lang'], stop=row['retain_count'])\n",
    "    # Save Japanese and Chinese chars with \\u200b char\n",
    "    if row['lang'] in ['/jpn', '/chi', '/kor', '/arm', '/urd']:\n",
    "        problems = df[(df.index.str.startswith('\\u200b') | df.index.str.endswith(\"\\u200b\"))]\n",
    "        problem_dfs.append(problems)\n",
    "    dfs.append(df)\n",
    "    \n",
    "asn_probchars = pd.concat(problem_dfs).groupby(level='token').sum().sort_values('count', ascending=False)\n",
    "print(asn_probchars)\n",
    "wordlist = pd.concat(dfs).groupby(level='token').sum().sort_values('count', ascending=False)\n",
    "print(\"Final wordlist using top N trim criteria: \", wordlist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing trim policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a 1000 word chunk starting at 'start'\n",
    "start = 0#9*10**5\n",
    "lang = '/eng'\n",
    "test_tokens = pd.read_hdf('/notebooks/data/final/final-sorted.h5', lang,\n",
    "                          start=start, stop=start+1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>outside</th>\n",
       "      <td>23254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>later</th>\n",
       "      <td>49994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>who</th>\n",
       "      <td>298154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factor</th>\n",
       "      <td>21430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without</th>\n",
       "      <td>81568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schools</th>\n",
       "      <td>28730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>associated</th>\n",
       "      <td>20396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid</th>\n",
       "      <td>30880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>47364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simply</th>\n",
       "      <td>22060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count\n",
       "token             \n",
       "outside      23254\n",
       "later        49994\n",
       "who         298154\n",
       "factor       21430\n",
       "without      81568\n",
       "schools      28730\n",
       "associated   20396\n",
       "aid          30880\n",
       "n            47364\n",
       "simply       22060"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly sample, so that you're not biased just to the \n",
    "# same ten tokens at the start of the list. Try re-running this cell\n",
    "test_tokens.sample(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing rules for removing likely junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (2020.7.14)\n",
      "(141924, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141924\n"
     ]
    }
   ],
   "source": [
    "# The re module is broken for hindi and similar characters, need to use regex\n",
    "!pip install regex\n",
    "import regex\n",
    "import numpy as np\n",
    "\n",
    "print(wordlist.shape)\n",
    "tokens = wordlist.index\n",
    "hyphenated = tokens.str.contains(r\"-\")\n",
    "#alpha = tokens.str.isalpha() # Faster, but bad for some languages\n",
    "alphaadv = tokens.map(lambda x: not not regex.search(\"^\\\\w+$\", x)).values.astype(np.bool_)\n",
    "number = tokens.str.contains(r\"^(£|$|€)?[\\d.,]+(st|nd|rd|th|s|C|F|c|m|°|¥)?$\")\n",
    "singlequote = tokens.str.contains(r\"[\\'’]\")\n",
    "abbr = tokens.str.contains(r\"^[^\\W\\d]([^\\W\\d]|\\.)+$\")\n",
    "endwithperiod = tokens.str.endswith('.')\n",
    "# This shows up for many asian characters, should be dealt with *before* wordlist is created\n",
    "blankchar = (tokens.str.startswith('\\u200b') | tokens.str.endswith(\"\\u200b\"))\n",
    "tlen = tokens.str.len()\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>है</th>\n",
       "      <td>6772484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>।</th>\n",
       "      <td>6255580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>के</th>\n",
       "      <td>5910544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>में</th>\n",
       "      <td>4718680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>की</th>\n",
       "      <td>3886890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count\n",
       "token         \n",
       "है     6772484\n",
       "।      6255580\n",
       "के     5910544\n",
       "में    4718680\n",
       "की     3886890"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_hdf('/notebooks/data/final/final-sorted.h5', 'hin', stop=1000000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "tokens = df.index\n",
    "hyphenated = tokens.str.contains(r\"-\")\n",
    "#alpha = tokens.str.isalpha() # Faster, but bad for some languages\n",
    "alphaadv = tokens.map(lambda x: not not regex.search(\"^\\\\w+$\", x)).values.astype(np.bool_)\n",
    "number = tokens.str.contains(r\"^(£|$|€)?[\\d.,]+(st|nd|rd|th|s|C|F|c|m|°|¥)?$\")\n",
    "singlequote = tokens.str.contains(r\"[\\'’]\")\n",
    "abbr = tokens.str.contains(r\"^[^\\W\\d]([^\\W\\d]|\\.)+$\")\n",
    "endwithperiod = tokens.str.endswith('.')\n",
    "# This shows up for many asian characters, should be dealt with *before* wordlist is created\n",
    "blankchar = (tokens.str.startswith('\\u200b') | tokens.str.endswith(\"\\u200b\"))\n",
    "tlen = tokens.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164570\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.bool_'>\n",
      "[ True  True  True ...  True  True False]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.bool_'>\n",
      "[False  True False ... False False  True]\n",
      "[ True False  True ...  True  True  True]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[ True  True  True ... False False  True]\n",
      "[ True  True  True ...  True  True  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['है,', 'हैं,', 'है।', 'हैं।', 'था,', 'हो,', 'नहीं,', '।\"', '1:',\n",
       "       '।।', 'थे,', 'में,', '``', 'थी,', 'है:', '),', 'है;', '\",', 'से,',\n",
       "       'कहा,', 'था।', 'है\"', 'को,', '(,', 'की,', '**', 'गया,', 'थे।',\n",
       "       ':,', 'का,', '।”', 'किया,', 'हूँ,', 'होगा,', '.:', 'के,', 'वही,',\n",
       "       'है)', 'थी।', 'भी,', '(२)', 'जी,', 'ही,', '।१', '(:', 'दिया,',\n",
       "       '.\"', '::', 'हों,', 'वि,', 'दु:ख', 'करना,', 'होता,', '(३)', 'गया।',\n",
       "       'गो,', '\"\"', 'दो,', 'पृ\"', '.)', 'बी,', 'रहे,', '?\"', '।२', 'हैं;',\n",
       "       '?”', 'होगी,', '।:', '***', 'गये,', 'दे,', 'लगा,', 'देखा,', ':)',\n",
       "       'जा,', '[.]', 'हैं:', 'नहीं।', 'रहा,', 'छो,', 'मैं,', 'किया।',\n",
       "       '(४)', 'शर्मा,', 'हाँ,', 'पु]', 'ने,', 'ले,', '(१)', 'सं,',\n",
       "       'बोला,', 'हो।', 'हैं\"', 'थीं,', 'सकता,', '\":', 'हु,', '()', 'हूं,',\n",
       "       'तो,'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(type(hyphenated))\n",
    "print(type(hyphenated[0]))\n",
    "print(~hyphenated)\n",
    "print(type(alphaadv))\n",
    "print(type(alphaadv[0]))\n",
    "print(~alphaadv)\n",
    "print(tlen >= 2)\n",
    "print(~endwithperiod)\n",
    "print(~singlequote)\n",
    "print(~number)\n",
    "print(~abbr)\n",
    "print(~blankchar)\n",
    "df[~hyphenated & ~alphaadv & (tlen >= 2) & ~endwithperiod & ~singlequote & ~number & ~abbr & ~blankchar].index.values[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected junk\n",
    "\n",
    "Top of the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141924\n",
      "141924\n",
      "141924\n",
      "141924\n",
      "141924\n",
      "141924\n",
      "141924\n",
      "141924\n",
      "141924\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>``</th>\n",
       "      <td>884644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>**</th>\n",
       "      <td>18386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>||</th>\n",
       "      <td>11450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0)</th>\n",
       "      <td>10826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1)</th>\n",
       "      <td>9064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>*,</th>\n",
       "      <td>8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>°C</th>\n",
       "      <td>6784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>—|</th>\n",
       "      <td>5074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2)</th>\n",
       "      <td>4144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>***</th>\n",
       "      <td>4084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count\n",
       "token        \n",
       "``     884644\n",
       "**      18386\n",
       "||      11450\n",
       "0)      10826\n",
       "1)       9064\n",
       "*,       8500\n",
       "°C       6784\n",
       "—|       5074\n",
       "2)       4144\n",
       "***      4084"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(wordlist))\n",
    "print(len(~hyphenated))\n",
    "print(len(~alphaadv))\n",
    "print(len(tlen >= 2))\n",
    "print(len(~endwithperiod))\n",
    "print(len(~singlequote))\n",
    "print(len(~number))\n",
    "print(len(~abbr))\n",
    "print(len(~blankchar))\n",
    "junk = wordlist[~hyphenated & ~alphaadv & (tlen >= 2) & ~endwithperiod & ~singlequote & ~number & ~abbr & ~blankchar]\n",
    "junk.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a random selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[20]</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>=\\</th>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100$&lt;</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>**『</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c/o</th>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>|T</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>+9</th>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>』､</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>、㎞の</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>・八</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count\n",
       "token       \n",
       "[20]      24\n",
       "=\\       110\n",
       "100$<     26\n",
       "**『       50\n",
       "c/o      132\n",
       "|T        22\n",
       "+9       130\n",
       "』､        34\n",
       "、㎞の       26\n",
       "・八        76"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "junk.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPN and CHI fix\n",
    "\n",
    "Any characters in the /jpn and /chi lists that have a non-breaking line space will be added, but with the id of the cleaned version. If there is no cleaned version, add one to the word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_candidate = wordlist[hyphenated | alphaadv | (tlen < 2) | endwithperiod | singlequote | number | abbr | blankchar].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [token, count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "prob_chars = asn_probchars.reset_index().query('token != \"\\u200b\"')\n",
    "print(prob_chars)\n",
    "prob_chars['broken'] =prob_chars['token']\n",
    "prob_chars['token'] = prob_chars['broken'].str.replace('\\u200b', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem characters that are not in the wordlist: add them (as fixed version)\n",
    "to_add = np.setdiff1d(prob_chars['token'].values, final_candidate['token'].values)\n",
    "new_lines = prob_chars[prob_chars['token'].isin(to_add)][['token', 'count']]\n",
    "final = pd.concat([final_candidate, new_lines])\\\n",
    "        .groupby('token', as_index=False).sum()\\\n",
    "        .sort_values('count', ascending=False)\\\n",
    "        .reset_index(drop=True)\\\n",
    "        .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210884\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0 unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0a69c2194e2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_chars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'broken'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mproblemchar_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_chars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'broken'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'broken'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mproblemchar_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[1;32m   4982\u001b[0m             )\n\u001b[1;32m   4983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4984\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4985\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
     ]
    }
   ],
   "source": [
    "# The indices to for the fixed characters. When we encounter the broken words in the dataset, we'll encode then\n",
    "# with the id for the fixed token\n",
    "print(len(final))\n",
    "print(len(prob_chars[['token','broken']]))\n",
    "problemchar_indices = pd.merge(final, prob_chars[['token', 'broken']], on='token')[['index','broken']]\n",
    "problemchar_indices.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save final list\n",
    "\n",
    "Also, save /jpn and /chi fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERWRITE MODE\n",
    "with pd.HDFStore('/notebooks/data/final/wordlist.h5', complib='blosc', mode='w', complevel=9) as store:\n",
    "    store.append('/final', final)\n",
    "    store.append('/fixes', problemchar_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test against dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader, utils\n",
    "\n",
    "# Two copies of the same dictionary, Laird and Lee's Webster's. They capitalize their words, so\n",
    "# I'm looking for capital words that occur in both.\n",
    "dicts = ['loc.ark:/13960/t84j1sb5j', 'loc.ark:/13960/t3xs70k06']\n",
    "paths = ['/notebooks/features/' + utils.id_to_rsync(volid) for volid in dicts]\n",
    "fr = FeatureReader(paths)\n",
    "tokenlist = []\n",
    "for vol in fr.volumes():\n",
    "    tokenlist += vol.tokens()\n",
    "\n",
    "tokens = pd.Series(tokenlist)\n",
    "# Grab capitalized letters\n",
    "dictionary_words = tokens[tokens.str.contains(r\"^[A-Z][A-Z\\-]*$\")].value_counts()\n",
    "shortlist = dictionary_words[dictionary_words > 1].index.str.lower().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_final_lower = final['token'].str.lower().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "extradictwords = np.setdiff1d(shortlist, unique_final_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12739          monetized\n",
       "20213       variableness\n",
       "10395          humidness\n",
       "5548         crimination\n",
       "16872    sensitive-plant\n",
       "17197        singlestick\n",
       "19037    through-lighted\n",
       "6386           denizened\n",
       "8891                fisc\n",
       "15301          pythoness\n",
       "7233        disregardful\n",
       "14772             posset\n",
       "2800             bordage\n",
       "5481           craziness\n",
       "19470         tree-nymph\n",
       "18474         synecdoche\n",
       "10256        hobby-horse\n",
       "20995             yauped\n",
       "17525        soliloquize\n",
       "11875           legories\n",
       "18342              surfy\n",
       "11011            inglobe\n",
       "4909         conjecturer\n",
       "6798     dictatorialness\n",
       "1027             apogean\n",
       "10427            huskies\n",
       "13706        overtrading\n",
       "18569           tameless\n",
       "18853          terranean\n",
       "2502             blabbed\n",
       "8555       expectorative\n",
       "1069         appeasingly\n",
       "9950      hair-splitting\n",
       "14618               plic\n",
       "16119             rifler\n",
       "13545          osteology\n",
       "5776         currishness\n",
       "9975           hall-mark\n",
       "20618           war-path\n",
       "19185                tng\n",
       "10258            hobnail\n",
       "15141        providently\n",
       "3061       brutalization\n",
       "202            actuaries\n",
       "7964               effem\n",
       "11130          insurable\n",
       "2656              bluffy\n",
       "7395                dobe\n",
       "10274        hoggishness\n",
       "18320        suppository\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(extradictwords).sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46591, 21104, 0.5470369813912559)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortlist.shape[0], extradictwords.shape[0], 1-extradictwords.shape[0]/shortlist.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/notebooks/data/final/wordlist.h5')\n",
    "df.to_csv('/notebooks/data/final/wordlist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junk filter testing, here be dragons\n",
    "\n",
    "Keeping this here as an example of how I tested various matching criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eng_2m = dd.read_hdf('/notebooks/data/final/final-sorted.h5', '/eng', stop=2000000).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "tokens = eng_2m.index\n",
    "alpha = tokens.str.isalpha()\n",
    "digit = tokens.str.isdigit()\n",
    "tlen = tokens.str.len()\n",
    "endwithperiod = tokens.str.endswith('.')\n",
    "quotes = (tokens.str.startswith('\"') | tokens.str.endswith('\"')) # | tokens.str.startswith('\\'') | tokens.str.endswith('\\''))\n",
    "endash = (tokens.str.startswith('—') | tokens.str.endswith('—'))\n",
    "punccount = tokens.str.count('[\\W]')\n",
    "repeating = tokens.str.contains(r\"(([\\w\\W])\\2{3,})\")\n",
    "repeatingdigit = tokens.str.contains(r\"((\\d)\\2{3,})\")\n",
    "abbr = tokens.str.contains(r\"^[^\\W\\d]([^\\W\\d]|\\.)+$\")\n",
    "singlequote = tokens.str.contains(r\"\\'\")\n",
    "hyphenated = tokens.str.contains(r\"-\")\n",
    "number = tokens.str.contains(r\"^(£|$|€)?[\\d.,]+(st|nd|rd|th|s|C|F|c|m|°|¥)?$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-RRB-</th>\n",
       "      <td>1477950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-LRB-</th>\n",
       "      <td>1441984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>''</th>\n",
       "      <td>916482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>``</th>\n",
       "      <td>792328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'s</th>\n",
       "      <td>600158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--</th>\n",
       "      <td>545330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-RSB-</th>\n",
       "      <td>285958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-LSB-</th>\n",
       "      <td>284386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n't</th>\n",
       "      <td>129732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'d</th>\n",
       "      <td>37548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count\n",
       "token         \n",
       "-RRB-  1477950\n",
       "-LRB-  1441984\n",
       "''      916482\n",
       "``      792328\n",
       "'s      600158\n",
       "--      545330\n",
       "-RSB-   285958\n",
       "-LSB-   284386\n",
       "n't     129732\n",
       "'d       37548"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_2m[~alpha & (tlen > 1) & ~endwithperiod & (punccount >= 1)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``\tहै\t°C\t**\t2d\tके\t2b\t2a\t+1\tतो\tभी\tand/or\t3d\tकि\tमें\tइति\t),\tनहीं\t»,\t.)\t।०\tको\t3D\tही\t6d\t.;\t3b\t1a\tहैं\tहै,\t***\tकी\tT3\tहो\t.\"\tRD&D\tC02\t2D\tH2\tका\tसे\t9/11\t+2\t.:\tमैं\tलिए\t1/2\t````\t:,\ta2\t1b\tA2\tथा\tR2\tजो\tx1\t>>\t0)\tS3\t1रि०\tm/s\tx2\t9d\t1n\tP1\tकोई\tS2\ta1\tn2\t3a\t\",\t5»\tP2\tA1\tUS$\ta>\tकहा\tभाव\tलोग\ti2\tगया\tअपने\tउसे\tAISI4140\tR&D\tहैं,\tता\t.‘\tT2\ts2\tN2\tX2\tN0\t??\t2»\t0»\tकुछ\tकाम\tकिया\tV2\tC2\tवे\t._\t,\"\tहुए\tv2\tक्या\tया\t2B\tt2\tt0\t?!\t^^\tबात\t<<\t£ηβ\tदिया\t°F\tचाहिए\tअपि\tI1\tm2\tये\tरहा\tमुझे\tप्र\tL2\tराम\tR1\tI2\t3B\tफिर\t1r\tसाथ\tरूप\tगये\tथी\t+00\tJ3\tकरते\tबाबा\t```\t5a\tΟΟ*0\tउसके\tQR/E\tमेरे\t1п\tI0\tA3\t==\tI4\tS1\tजाता\tB1\tCO2\tv1\ti6\tअपनी\tथे\t!!\t<I>\t\"\"\tJ2\tहोता\tहुआ\tप्त\te2\tM2\t2n\tमैंने\t<f>\tE2\tB2\t.—\tk2\tC0\tr2\t1:\t<t>\tt1\t०त्\tमेरी\t‘_\tC1\tl2\t1/3\tहूँ\tST363\tदो\tजा\this/her\t8vo\tअति\tH20\tT1\t23d\t2A\tरहे\tN/A\t2S\tL1\tकारण\tl0\tr1\tm3\tn/a\tहुई\ti8\tf2\t«44\tF2\tp2\tU2\tकरने\tC3\t(1\tपाते\t••\tउन्हें\t+3\t****\t*»\tऐसा\tपास\tV0\tQ1\tमेरा\tनाम\tM3\tकरना\t.),\tMoS2\tl1\t6a\tजैसे\tE1\t१रि०\tM1\tX1\t2i\tयहाँ\tTests/A\tK2\ts1\tS0\t7a\tn0\t,—\tने\tहमारे\tb2\t12mo\ti860\t1l\tI5\tc2\tले\t2003a\te1\tV1\tबोले\t3i\t//\t०ई\tF1\ty1\ty2\tहमें\tपहले\tG2\t);\t1:1\t4a\t1/4\t0O\th2\t2003b\tदे\t<j>\t3/4\t■o\t1t\tS4\tQ2\tरि०\t+4\t3A\t7b\t\\\\\t+5\tराजा\tतुम\tp1\t||\tउसका\thttp://www\tD2\tx0\tबागी\n"
     ]
    }
   ],
   "source": [
    "a = eng_2m[~hyphenated & ~alpha & (tlen >= 2) & ~endwithperiod & ~singlequote & ~number & ~abbr]\n",
    "print(\"\\t\".join(a[:300].index.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pr.p</th>\n",
       "      <td>23846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>___</th>\n",
       "      <td>2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>____</th>\n",
       "      <td>1512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n.pl</th>\n",
       "      <td>1192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F.C</th>\n",
       "      <td>1098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H.S</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J.A</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ipr.p</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i.xix</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O.TP</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       count\n",
       "token       \n",
       "pr.p   23846\n",
       "___     2550\n",
       "____    1512\n",
       "n.pl    1192\n",
       "F.C     1098\n",
       "...      ...\n",
       "H.S       20\n",
       "J.A       20\n",
       "Ipr.p     20\n",
       "i.xix     20\n",
       "O.TP      20\n",
       "\n",
       "[203 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_2m[~alpha & abbr & ~endwithperiod & (tlen > 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "abbr = tokens.str.contains(r\"^[^\\W\\d]([^\\W\\d]|\\.)+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0028315"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_2m[~hyphenated & ~abbr & ~alpha & (tlen >= 2) & ~endwithperiod & ~singlequote & ~number].shape[0] /2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000000000000</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000000000</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000000000000</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count\n",
       "token                  \n",
       "000000000000         54\n",
       "00000000000          52\n",
       "0000000000000        22\n",
       "0000000000000000     22"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Longest digit-only values\n",
    "eng_2m[digit & (tlen > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>2322422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>4937172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>7289634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that</td>\n",
       "      <td>1130682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>which</td>\n",
       "      <td>439120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>should</td>\n",
       "      <td>109122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>between</td>\n",
       "      <td>108458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>American</td>\n",
       "      <td>47712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>different</td>\n",
       "      <td>55256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>University</td>\n",
       "      <td>44640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>information</td>\n",
       "      <td>36738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>particularly</td>\n",
       "      <td>15832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>International</td>\n",
       "      <td>14780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>administration</td>\n",
       "      <td>8090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>characteristics</td>\n",
       "      <td>8558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>responsibilities</td>\n",
       "      <td>2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>социалистического</td>\n",
       "      <td>1760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>производительности</td>\n",
       "      <td>738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Naturwissenschaften</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>некапиталистического</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>сельскохозяйственного</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>microelectromechanical</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>silametallacyclobutanes</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         token    count\n",
       "chars                                  \n",
       "1                            a  2322422\n",
       "2                           of  4937172\n",
       "3                          the  7289634\n",
       "4                         that  1130682\n",
       "5                        which   439120\n",
       "6                       should   109122\n",
       "7                      between   108458\n",
       "8                     American    47712\n",
       "9                    different    55256\n",
       "10                  University    44640\n",
       "11                 information    36738\n",
       "12                particularly    15832\n",
       "13               International    14780\n",
       "14              administration     8090\n",
       "15             characteristics     8558\n",
       "16            responsibilities     2750\n",
       "17           социалистического     1760\n",
       "18          производительности      738\n",
       "19         Naturwissenschaften      110\n",
       "20        некапиталистического      126\n",
       "21       сельскохозяйственного      126\n",
       "22      microelectromechanical       48\n",
       "23     silametallacyclobutanes       34"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Long words\n",
    "pd.set_option('display.float_format', lambda x: '%.0f' % x)\n",
    "longwords = eng_2m[alpha].reset_index().copy()\n",
    "longwords['chars'] = longwords['token'].str.len()\n",
    "longwords.head(1)\n",
    "longwords.groupby('chars').apply(lambda x: x.sort_values('count').iloc[-1] )[['token', 'count']][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21718, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_nonalpha = eng_2m[~eng_2m.index.str.isalpha()]\n",
    "eng_nonalpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19192, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_nonalphanumeric = eng_nonalpha[~eng_nonalpha.index.str.isdigit()]\n",
    "eng_nonalphanumeric.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
